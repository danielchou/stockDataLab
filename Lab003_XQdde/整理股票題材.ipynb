{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "def json_to_csv(json_file_path, csv_file_path):\n",
    "    # è®€å– JSON æª”æ¡ˆï¼ŒæŒ‡å®šç·¨ç¢¼ç‚º utf-8\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # å¦‚æœ JSON æ•¸æ“šæ˜¯ä¸€å€‹åˆ—è¡¨ï¼Œå‰‡å–ç¬¬ä¸€å€‹å…ƒç´ çš„éµä½œç‚º CSV çš„æ¨™é¡Œ\n",
    "    if isinstance(data, list) and len(data) > 0:\n",
    "        keys = data[0].keys()\n",
    "\n",
    "        # å¯«å…¥ CSV æª”æ¡ˆ\n",
    "        with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=keys)\n",
    "\n",
    "            # å¯«å…¥æ¨™é¡Œ\n",
    "            writer.writeheader()\n",
    "\n",
    "            # å¯«å…¥æ•¸æ“š\n",
    "            for row in data:\n",
    "                # è™•ç†ç‰¹å®šæ¬„ä½\n",
    "                for key in row:\n",
    "                    if isinstance(row[key], str):\n",
    "                        try:\n",
    "                            # ä½¿ç”¨ ast.literal_eval å°‡å­—ç¬¦ä¸²è½‰æ›ç‚ºåˆ—è¡¨\n",
    "                            list_data = ast.literal_eval(row[key])\n",
    "                            if isinstance(list_data, list) and len(list_data) > 0:\n",
    "                                row[key] = list_data[0]\n",
    "                        except (ValueError, SyntaxError):\n",
    "                            pass\n",
    "\n",
    "                writer.writerow(row)\n",
    "\n",
    "        print(f\"JSON æª”æ¡ˆå·²æˆåŠŸè½‰æ›ç‚º CSV æª”æ¡ˆ: {csv_file_path}\")\n",
    "    else:\n",
    "        print(\"JSON æª”æ¡ˆæ ¼å¼ä¸æ­£ç¢ºæˆ–ç‚ºç©ºã€‚\")\n",
    "\n",
    "# ä½¿ç”¨ç¯„ä¾‹\n",
    "json_file_path = r'D:\\project\\stockDataLab\\Lab\\data\\\\webJson\\\\stock_category.json'\n",
    "csv_file_path = r'D:\\project\\stockDataLab\\Lab\\data\\\\webJson\\\\stock_category.csv'\n",
    "json_to_csv(json_file_path, csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def csv_to_json(csv_file_path, json_file_path):\n",
    "    # è®€å– CSV æª”æ¡ˆ\n",
    "    data = []\n",
    "    with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            # ç§»é™¤ 'name' æ¬„ä½\n",
    "            print(row)\n",
    "            row.pop('\\ufeffname', None)\n",
    "            # å°‡ 'id' æ¬„ä½è½‰æ›ç‚ºæ•´æ•¸\n",
    "            if 'id' in row:\n",
    "                row['id'] = int(row['id'])\n",
    "            data.append(row)\n",
    "\n",
    "    # å¯«å…¥ JSON æª”æ¡ˆ\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, separators=(',', ':'))\n",
    "\n",
    "    print(f\"CSV æª”æ¡ˆå·²æˆåŠŸè½‰æ›ç‚º JSON æª”æ¡ˆ: {json_file_path}\")\n",
    "\n",
    "# ä½¿ç”¨ç¯„ä¾‹\n",
    "csv_file_path = r'D:\\project\\stockDataLab\\Lab\\data\\\\webJson\\\\stock_group.csv'\n",
    "json_file_path = r'D:\\project\\stockDataLab\\Lab\\data\\\\webJson\\\\stock_category.json'\n",
    "csv_to_json(csv_file_path, json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¿å¡Šåœ–è¨­è¨ˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name    id  cg\n",
      "392      å‡Œç¾¤  2453  è³‡å®‰\n",
      "588      é›¶å£¹  3029  è³‡å®‰\n",
      "838   å®‰ç‘-KY  3664  è³‡å®‰\n",
      "1311     ç²¾èª   6214  è³‡å®‰\n",
      "1454     æ™®é´»  6590  è³‡å®‰\n",
      "1494   å®‰ç¢è³‡è¨Š  6690  è³‡å®‰\n",
      "1555   å‰åº·ç§‘æŠ€  6865  è³‡å®‰\n",
      "1565    èµ°è‘—ç§  6902  è³‡å®‰\n"
     ]
    }
   ],
   "source": [
    "#ä¿®æ”¹è‚¡ç¥¨é¡Œæ\n",
    "import pandas as pd\n",
    "\n",
    "csv_file = r'webJson\\stock_subject.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "# # çµ¦å®šçš„æ›¿æ›è¦å‰‡\n",
    "# replacement_rule = \"PBC-HDI:æ¬£èˆˆã€è¯é€šã€å¥é¼ã€è¯èŒ‚ã€å®šç©æŠ•æ§\"\n",
    "# replacement_rule = \"AIæ©Ÿå™¨è¦–è¦º:ä½³èƒ½ã€åœ“å‰›ã€æ‰€ç¾…é–€ã€é”æ˜\"\n",
    "# replacement_rule = \"IPæ™ºè²¡:å‰µæ„ã€æ™ºåŸã€åŠ›æ—ºã€æ™¶å¿ƒç§‘ã€M31ã€ä¸–èŠ¯-KY\"\n",
    "# replacement_rule = \"æ¿å¡é¡¯å¡:è¯æ“ã€æ˜ æ³°ã€æ‰¿å•Ÿã€éº—è‡ºã€æ’¼è¨Šã€é’é›²\"\n",
    "# replacement_rule = \"æ•£ç†±é¡¯å¡:å‹•åŠ›-KYã€å¥‡é‹ã€å°¼å¾—ç§‘è¶…çœ¾\"\n",
    "# replacement_rule = \"Switch2:èŒ‚é”ã€å‰µæƒŸã€åŸç›¸ã€æ—ºå®ã€ç‘æ˜±ã€å‰è©®é›»\"\n",
    "# replacement_rule = \"æ¶²å†·ä¸‰é›„:å¥‡é‹ã€é›™é´»ã€å»ºæº–\"\n",
    "# replacement_rule = \"ä¹³è† é›™é›„:å—å¸ã€ç”³è±\"\n",
    "# replacement_rule = \"CoWoSè¨­å‚™:å¼˜å¡‘ã€è¾›è€˜ã€è¬æ½¤ã€å¿—è–ã€å‡è±ªã€å‡è¯\"\n",
    "# replacement_rule = \"CPOçŸ½å…‰å­:è¯éˆã€è¯äºã€å…‰è–ã€å‰µå¨ã€è¯å…‰é€šã€ä¸Šè©®ã€è¯æ˜Ÿå…‰\"\n",
    "# replacement_rule = \"FOPLP:é‘«ç§‘ã€å‹å¨ç§‘ã€æ±æ·ã€ç¾¤å‰µ\"\n",
    "# replacement_rule = \"ç„¡äººæ©Ÿ:é›·è™ã€ä¸­å…‰é›»ã€åœ“å‰›ã€ç¾…æ˜‡ã€æ‰€ç¾…é–€\"\n",
    "# replacement_rule = \"BBU:ç³»çµ±é›»ã€ç§‘é¢¨ã€å…¨æ¼¢ã€è¥¿å‹ã€é †é”ã€èˆˆèƒ½é«˜ã€é‡‘å±±é›»ã€æ—¥é›»è²¿ã€æ–°ç››åŠ›ã€åŠ ç™¾è£•ã€é•·åœ’ç§‘\"\n",
    "replacement_rule = \"è³‡å®‰:å®‰ç¢è³‡è¨Šã€ç²¾èª ã€é›¶å£¹ã€æ™®é´»ã€å®‰ç‘-KYã€å‰åº·ç§‘æŠ€ã€èµ°è‘—ç§ã€å‡Œç¾¤\"\n",
    "# è§£ææ›¿æ›è¦å‰‡\n",
    "new_category, codes = replacement_rule.split(':')\n",
    "codes = set(map(str, codes.split('ã€')))\n",
    "\n",
    "# print(new_category, codes)\n",
    "\n",
    "# # æ›´æ–° DataFrame ä¸­çš„ç¬¬ä¸‰æ¬„ä½\n",
    "# df.loc[df['id'].astype(int).isin(codes), 'cg'] = new_category\n",
    "# print(df[df['id'].isin(codes)])\n",
    "\n",
    "# æ›´æ–° DataFrame ä¸­çš„ç¬¬ä¸‰æ¬„ä½\n",
    "df.loc[df['name'].isin(codes), 'cg'] = new_category\n",
    "print(df[df['name'].isin(codes)])\n",
    "\n",
    "# å°‡æ›´æ–°å¾Œçš„ DataFrame å¯«å›åˆ° CSV æ–‡ä»¶\n",
    "df.to_csv(csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n"
     ]
    }
   ],
   "source": [
    "# è®€å–æ–‡å­—æª”æ¡ˆä¸­çš„è‚¡ç¥¨ä»£ç¢¼\n",
    "with open(r'stock/0.txt', 'r') as file:\n",
    "    stock_ids = file.read().strip().split(',')\n",
    "\n",
    "# éæ¿¾æ‰ç©ºå­—ä¸²ä¸¦å°‡è‚¡ç¥¨ä»£ç¢¼è½‰æ›ç‚ºæ•´æ•¸\n",
    "stock_ids = [int(stock_id) for stock_id in stock_ids if stock_id]\n",
    "\n",
    "# å°‡è‚¡ç¥¨ä»£ç¢¼æ’åº\n",
    "stock_ids = sorted(stock_ids)\n",
    "\n",
    "# å°‡æ’åºå¾Œçš„è‚¡ç¥¨ä»£ç¢¼è½‰æ›å›å­—ä¸²\n",
    "sorted_stock_ids = ','.join(map(str, stock_ids))\n",
    "\n",
    "# è¼¸å‡ºæ’åºå¾Œçš„è‚¡ç¥¨ä»£ç¢¼\n",
    "print(len(stock_ids))\n",
    "\n",
    "# å°‡æ’åºå¾Œçš„è‚¡ç¥¨ä»£ç¢¼å¯«å›æ–‡å­—æª”æ¡ˆ\n",
    "with open(r'stock/sorted.txt', 'w') as file:\n",
    "    file.write(sorted_stock_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### åˆ†åˆ‡å‡ºäº”å€‹å°æª”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®€å–æ–‡å­—æª”æ¡ˆä¸­çš„è‚¡ç¥¨ä»£ç¢¼\n",
    "with open(r'stock/sorted.txt', 'r') as file:\n",
    "    stock_ids = file.read().strip().split(',')\n",
    "\n",
    "# éæ¿¾æ‰ç©ºå­—ä¸²ä¸¦å°‡è‚¡ç¥¨ä»£ç¢¼è½‰æ›ç‚ºæ•´æ•¸\n",
    "stock_ids = [int(stock_id) for stock_id in stock_ids if stock_id]\n",
    "\n",
    "# å°‡è‚¡ç¥¨ä»£ç¢¼æ’åº\n",
    "stock_ids = sorted(stock_ids)\n",
    "\n",
    "# è¨ˆç®—æ¯å€‹æª”æ¡ˆæ‡‰è©²åŒ…å«çš„è‚¡ç¥¨ä»£ç¢¼æ•¸é‡\n",
    "num_files = 5\n",
    "total_stock_ids = len(stock_ids)\n",
    "average_stock_ids_per_file = total_stock_ids // num_files\n",
    "remainder = total_stock_ids % num_files\n",
    "\n",
    "import random\n",
    "\n",
    "# ç”Ÿæˆæ¯å€‹æª”æ¡ˆçš„è‚¡ç¥¨ä»£ç¢¼æ•¸é‡ï¼Œç¢ºä¿ç›¸å·®å¤§ç´„20ç­†\n",
    "file_sizes = []\n",
    "for i in range(num_files):\n",
    "    size = average_stock_ids_per_file + (1 if i < remainder else 0)\n",
    "    size += random.randint(-15, 15)  # éš¨æ©Ÿèª¿æ•´å¤§å°ï¼Œç¢ºä¿ç›¸å·®å¤§ç´„20ç­†\n",
    "    file_sizes.append(size)\n",
    "\n",
    "# èª¿æ•´æª”æ¡ˆå¤§å°ï¼Œç¢ºä¿ç¸½å’Œç­‰æ–¼ç¸½è‚¡ç¥¨æ•¸é‡\n",
    "while sum(file_sizes) != total_stock_ids:\n",
    "    if sum(file_sizes) < total_stock_ids:\n",
    "        file_sizes[random.randint(0, num_files - 1)] += 1\n",
    "    else:\n",
    "        file_sizes[random.randint(0, num_files - 1)] -= 1\n",
    "\n",
    "# å°‡è‚¡ç¥¨ä»£ç¢¼åˆ†é…åˆ°äº”å€‹æª”æ¡ˆä¸­\n",
    "file_index = 1\n",
    "current_index = 0\n",
    "\n",
    "for size in file_sizes:\n",
    "    # å–å¾—ç•¶å‰æª”æ¡ˆçš„è‚¡ç¥¨ä»£ç¢¼\n",
    "    current_stock_ids = stock_ids[current_index:current_index + size]\n",
    "\n",
    "    # å°‡è‚¡ç¥¨ä»£ç¢¼è½‰æ›å›å­—ä¸²\n",
    "    current_stock_ids_str = ','.join(map(str, current_stock_ids))\n",
    "\n",
    "    # å¯«å…¥æª”æ¡ˆ\n",
    "    with open(f'stock/Id{file_index}.txt', 'w') as file:\n",
    "        file.write(current_stock_ids_str)\n",
    "\n",
    "    # æ›´æ–°ç´¢å¼•\n",
    "    current_index += size\n",
    "    file_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n",
      "  |AMDä¼ºæœå™¨|CPOçŸ½å…‰å­|CoWoSè¨­å‚™|DRAMè£½é€ |FOPLP|GPUæ¸¬è©¦å°è£|IP|IPæ™ºè²¡|LEDå°è£æ¨¡çµ„|LEDæª¢æ¸¬è¨­å‚™|LEDç‡ˆå…·|LEDç”Ÿç”¢è£½ç¨‹æª¢æ¸¬|LEDç£Šæ™¶|LEDè»Šç‡ˆæ¨¡çµ„|LEDé©…å‹•ğŸ‡®ğŸ‡¨|MOSFET|PBC-HDI|PC|PCB|PCBIOS-åµŒå…¥å¼è»Ÿé«”|PCBç”Ÿç”¢è£½ç¨‹åŠæª¢æ¸¬è¨­å‚™|PCBé‘½é‡|Switch2|TGVå…ˆé€²å°è£|Type-C|ä¹³è† é›™é›„|ä¼‘é–’è»Šæ¥­|ä¼ºæœå™¨|ä½è»Œè¡›æ˜Ÿ|ä¿å¥é£Ÿå“|å¥èº«å™¨æ|å‚³å‹•å…ƒä»¶|å‚³ç”¢|å…ƒå®‡å®™|å…ˆé€²å°è£|å…‰å­¸|å…‰ç¢Ÿç‰‡|å…‰ç½©|å…‰é€šè¨Šè¨­å‚™|å†ç”Ÿé†«ç™‚|å†·å‡ç©ºèª¿|åŒ–å·¥|åŠå°|å°åˆ·|å°è¡¨æ©Ÿ|å°ç©é›»è¨­å‚™|å°ç©é›»è¨­å‚™AHFæ°£é«”|åŸºç¤å·¥ç¨‹|å¡‘åŒ–|å¤§æ•¸æ“šæ•´åˆ|å¤ªé™½èƒ½|å¨›æ¨‚æœå‹™æ¥­|å®‰å…¨ç›£æ§ç³»çµ±|å®¶é›»|å°ç·šæ¶|å±…å®¶ç”Ÿæ´»|å·¥å…·æ©Ÿ|å·¥æ¥­é›»è…¦|å·¥ç¨‹æ‰¿æ”¬|å¹³é¢|å¹³é¢é¡¯ç¤ºå™¨|å»ºæ|å½±éŸ³æšè²å™¨|æ•£ç†±|æ•£ç†±é¡¯å¡|æ•£è£èˆªé‹|æ–‡å‰µ|æ—…éŠ|æ—…é¤¨|æ™¶åœ“è£½é€ |æ™ºæ…§é›»ç¶²|æ¿å¡é¡¯å¡|æ©¡è† è£½å“|æ°´æ³¥|æ°´è³‡æº|æ±½é›»å…±ç”Ÿ|æ²¹é›»ç‡ƒæ°£|æµ·é™¸ç©ºå¤§çœ¾é‹è¼¸|æµ·é™¸ç©ºè²¨é‹æ‰¿æ”¬|æ¶ˆè²»æ€§ğŸ‡®ğŸ‡¨|æ¶²å†·ä¸‰é›„|æ¸…æ½”ç”¨å“|ç‡Ÿå»º|ç‡Ÿé€ |ç»çº–å¸ƒ|ç’°ä¿|ç’°ä¿æ½”èƒ½|ç”ŸæŠ€|ç”Ÿç‰©æª¢æ¸¬|ç ·åŒ–éµ|ç¥ç›¾é›†åœ˜|ç³»çµ±æ•´åˆ|ç´¡ç¹”|ç¶ é›»|ç·šä¸ŠéŠæˆ²|ç¾è¶…å¾®MCSI|èšé…¯çº–ç¶­|èˆªå¤ªé€±é‚Š|èˆªç©º|è¡›æ˜Ÿé€šä¿¡|è¢«å‹•å…ƒä»¶|è£½è—¥|è§¸æ§é¢æ¿|è¨˜æ†¶é«”|è²¨æ«ƒèˆªé‹|è²¨æ«ƒé‹è¼¸|è²¿æ˜“ç™¾è²¨|è³‡å®‰|è»Šç”¨3Dç»ç’ƒ|è»Šç”¨æ©Ÿæ¢°å‚³å‹•|è»å·¥|è¾²æ¥­ç§‘æŠ€|é€šä¿¡|é€šä¿¡ç¶²è·¯|é€šä¿¡è¢«å‹•å…ƒä»¶|é€ ç´™|é€£æ¥å™¨|é‹å‹•å™¨æ|é‹å‹•ç§‘æŠ€|é†«ç™‚å™¨æ|é†«ç™‚è€—æ|é‡é›»|é‡‘å±¬åŠ å·¥ç”¨æ©Ÿæ¢°|é‡‘è|é‡‘èç§‘æŠ€|é‹¼éµ|é‹¼éµé¢¨é›»|éš±å‹çœ¼é¡|é›²ç«¯é‹ç®—|é›²ç«¯é‹ç®— |é›»å‹•è»Š|é›»å‹•è»Šé›»æ± |é›»å­å•†å‹™|é›»å­ç´™|é›»å­é©—è­‰åˆ†æ|é›»æ©Ÿ|é›»æ± æ¨¡çµ„|é›»æºç®¡ç†ğŸ‡®ğŸ‡¨|é›»çºœ|é›»é|é¢æ¿|éŸ³éŸ¿ä»£å·¥|é¢¨åŠ›ç™¼é›»|é£Ÿå“|é¤é£²é€£é–|é«˜çˆ¾å¤«çƒå…·æ¥­|é«˜é€Ÿå‚³è¼¸|ğŸ‡¦ğŸ‡®|ğŸ‡¦ğŸ‡®PC|ğŸ‡¦ğŸ‡®PCé›¶çµ„ä»¶é›»æ‰‡|ğŸ‡¦ğŸ‡®ä¼ºæœå™¨|ğŸ‡¦ğŸ‡®æ™ºæ…§è¨­å‚™|ğŸ‡¦ğŸ‡®æ©Ÿå™¨å­¸ç¿’|ğŸ‡¦ğŸ‡®æ©Ÿå™¨è¦–è¦º|ğŸ‡¦ğŸ‡®ç³»çµ±æ•´åˆ|ğŸ‡¦ğŸ‡®é‹ç®—è¨­å‚™|ğŸ‡¦ğŸ‡®é›»è…¦è¦–è¦º|ğŸ‡¦ğŸ‡®é ˜åŸŸè§£æ±ºæ–¹æ¡ˆ|ğŸ‡¦ğŸ‡®ğŸ¤–|ğŸ‡®ğŸ‡¨äºŒæ¥µé«”|ğŸ‡®ğŸ‡¨å…‰æºç®¡ç†|ğŸ‡®ğŸ‡¨å…‰é€šè¨Š|ğŸ‡®ğŸ‡¨åŒ–å­¸å“|ğŸ‡®ğŸ‡¨åŸºæ¿|ğŸ‡®ğŸ‡¨å°è£æ¸¬è©¦|ğŸ‡®ğŸ‡¨å°ç·šæ¶|ğŸ‡®ğŸ‡¨å¾®æ§åˆ¶å™¨|ğŸ‡®ğŸ‡¨æ¨¡çµ„|ğŸ‡®ğŸ‡¨æª¢æ¸¬è¨­å‚™|ğŸ‡®ğŸ‡¨ç‰¹ç”¨åŒ–å­¸|ğŸ‡®ğŸ‡¨ç¶²è·¯é€šè¨Š|ğŸ‡®ğŸ‡¨è£½ç¨‹è¨­å‚™|ğŸ‡®ğŸ‡¨è¨˜æ†¶é«”æ§åˆ¶|ğŸ‡®ğŸ‡¨è¨­å‚™å» |ğŸ‡®ğŸ‡¨è¨­è¨ˆ|ğŸ‡®ğŸ‡¨è¼¸å‡ºå…¥ä»‹é¢|ğŸ‡®ğŸ‡¨é€šè·¯|ğŸ‡®ğŸ‡¨é¢æ¿é©…å‹•|ğŸ‡®ğŸ‡¨é¡¯ç¤ºé©…å‹•|ğŸ’»ä¸»æ©Ÿæ¿|ğŸ’»å‘¨é‚Š|ğŸ’»æ©Ÿæ§‹æ¨ç´|ğŸ’»æ©Ÿæ®¼|ğŸ’»ç›£æ§ç³»çµ±|ğŸ’»é€£æ¥ç·š|ğŸ’»é›»æºä¾›æ‡‰å™¨|ğŸš—|ğŸš—PC|ğŸš—ä¿éšªæ¡¿|ğŸš—çµ„è£|ğŸš—è»Šç‡ˆ|ğŸš—è¼ªèƒ|ğŸš—é€šä¿¡ç¶²è·¯|ğŸš—éŠ·å”®|ğŸš—é›¶çµ„ä»¶|ğŸš—é›¶çµ„ä»¶AM|ğŸš—é›»æ©Ÿ|ğŸš—é¢æ¿\n"
     ]
    }
   ],
   "source": [
    "#ä¿®æ”¹è‚¡ç¥¨é¡Œæ\n",
    "import pandas as pd\n",
    "\n",
    "csv_file = r'webJson\\stock_subject.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.dropna()\n",
    "\n",
    "se= df[\"cg\"].drop_duplicates().sort_values().tolist()\n",
    "print(len(se))\n",
    "print('|'.join(se))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
